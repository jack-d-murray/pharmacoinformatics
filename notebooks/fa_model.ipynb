{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986ed013-4f93-4a2b-bcb7-6b87c89de33e",
   "metadata": {},
   "source": [
    "# 4. Prediction of Human Oral Fraction Absorbed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8abf2-693c-4ca0-b8fa-7e3dd9f2ea5a",
   "metadata": {},
   "source": [
    "This notebook describes some exploratory data analysis and the construction of predictive models of human oral fraction absorbed. Decision tree classifiers and tree-based ensemble methods (AdaBoostClassifier, RandomForestClassifier, and GradientBoostingClassifier) were considered due to their established success in ADMET prediction tasks, their ability to perform explicit/implicit feature selection, and because they are universal function approximators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Cheminformatics libraries\n",
    "from descriptastorus.descriptors.DescriptorGenerator import MakeGenerator\n",
    "import mordred\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, inchi, MACCSkeys, AllChem, DataStructs, PandasTools, PyMol, Lipinski\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem.PandasTools import LoadSDF\n",
    "from rdkit.DataStructs import ConvertToNumpyArray\n",
    "\n",
    "# Data manipulation and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate, KFold, StratifiedKFold\n",
    "\n",
    "# Other\n",
    "import ast\n",
    "import pickle\n",
    "from scipy import spatial as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read necessary .csv files\n",
    "original_df_ = pd.read_csv(\"../csv_files/final_master_df.csv\", index_col = 0)\n",
    "actives_df = pd.read_csv(\"../csv_files/adding_chembl_to_actives.csv\", index_col = 0)\n",
    "\n",
    "# Isolating oral drugs\n",
    "original_df = original_df_[[\"actives_in_dosage_form\",\"Dosage Form\", \"Route\", \"Fa1\", \"Fa2\", \"Fa3\", \"Fa4\", \"F1\", \"F2\", \"F3\", \"F4\", \"Tmax1\", \"Tmax2\", \"Tmax3\", \"Tmax4\", \"Excipients_Final\"]]\n",
    "original_df.replace(\"Blank\", np.nan, inplace = True)\n",
    "intermediate_df = original_df[original_df[\"Route\"]==\"Oral\"]\n",
    "\n",
    "# Convert multiple columns into arrays in a single column so .explode() method can be used\n",
    "intermediate_df[\"actives_in_dosage_form\"] = intermediate_df[\"actives_in_dosage_form\"].apply(lambda x: x.upper().strip().split(\",\"))\n",
    "intermediate_df[\"Fa\"] = intermediate_df.apply(lambda row: [row[\"Fa1\"], row[\"Fa2\"], row[\"Fa3\"], row[\"Fa4\"]], axis=1)\n",
    "intermediate_df[\"F\"] = intermediate_df.apply(lambda row: [row[\"F1\"], row[\"F2\"], row[\"F3\"], row[\"F4\"]], axis=1)\n",
    "intermediate_df[\"Tmax\"] = intermediate_df.apply(lambda row: [row[\"Tmax1\"], row[\"Tmax2\"], row[\"Tmax3\"], row[\"Tmax4\"]], axis=1)\n",
    "intermediate_df.drop([\"Fa1\", \"Fa2\", \"Fa3\", \"Fa4\", \"F1\", \"F2\", \"F3\", \"F4\", \"Tmax1\", \"Tmax2\", \"Tmax3\", \"Tmax4\"], axis=1, inplace=True)\n",
    "\n",
    "# Use the .explode() method on multiple columns\n",
    "def prepare_for_explode(row):\n",
    "    length = len(row[\"actives_in_dosage_form\"])\n",
    "    row[\"Fa\"] = row[\"Fa\"][:length]\n",
    "    row[\"F\"] = row[\"F\"][:length]\n",
    "    row[\"Tmax\"] = row[\"Tmax\"][:length]\n",
    "    return row\n",
    "intermediate_df = intermediate_df.apply(prepare_for_explode, axis=1)\n",
    "intermediate_df = intermediate_df.explode([\"actives_in_dosage_form\", \"Fa\", \"F\", \"Tmax\"])\n",
    "intermediate_df[\"actives_in_dosage_form\"] = intermediate_df[\"actives_in_dosage_form\"].apply(lambda x: x.strip())\n",
    "actives_df = actives_df[[\"active_PSS\",\"pss_inchi\", \"IsomericSMILES\", \"ps_smiles\", \"ps_num_drugs\", \"p_smiles\", \"p_inchi\",\"p_inchikey\"]].drop_duplicates()\n",
    "actives_df.rename(columns = {\"active_PSS\": \"actives_in_dosage_form\"}, inplace=True)\n",
    "\n",
    "# Merge into one final DataFrame and convert strings to floats\n",
    "final_df = pd.merge(intermediate_df, actives_df, how=\"left\", on=\"actives_in_dosage_form\")\n",
    "final_df.drop_duplicates(inplace=True)\n",
    "final_df[\"Fa\"], final_df[\"F\"], final_df[\"Tmax\"] = final_df[\"Fa\"].astype(float), final_df[\"F\"].astype(float), final_df[\"Tmax\"].astype(float)\n",
    "final_df.reset_index(inplace=True)\n",
    "\n",
    "# Remove multicomponent parent molecules\n",
    "final_df[\"multicomponent\"] = final_df[\"p_smiles\"].apply(lambda x: \".\" in x)\n",
    "filtered_df = final_df[final_df[\"multicomponent\"] == False]\n",
    "filtered_df_f = filtered_df[filtered_df[\"F\"].notna()].drop_duplicates(subset = [\"p_inchikey\"])\n",
    "filtered_df = filtered_df[filtered_df[\"Fa\"].notna()].drop_duplicates(subset = [\"p_inchikey\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380a662-8185-46b7-b13d-18960bbea1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframes for EDA plots - Route\n",
    "route_counts = original_df[\"Route\"].value_counts()\n",
    "other_count = route_counts[route_counts < 11].sum()\n",
    "route_counts = route_counts[route_counts >= 11]\n",
    "route_counts[\"Other\"] = other_count\n",
    "oral_counts = original_df[original_df[\"Route\"] == \"Oral\"][\"Dosage Form\"].value_counts()\n",
    "other_dosages = oral_counts[oral_counts < 40].sum()\n",
    "oral_counts = oral_counts[oral_counts >= 40]\n",
    "oral_counts[\"Other\"] = other_dosages.sum()\n",
    "route_counts_df = route_counts.replace(\"Oral\", 0).to_frame()\n",
    "for form, count in oral_counts[::-1].items():\n",
    "    route_counts_df[form] = 0\n",
    "route_counts_df[\"count\"][\"Oral\"] = 0\n",
    "route_counts_df[\"Tablet\"][\"Oral\"] = oral_counts[\"Tablet\"]\n",
    "route_counts_df[\"Hard Capsule\"][\"Oral\"] = oral_counts[\"Hard Capsule\"]\n",
    "route_counts_df[\"Solution\"][\"Oral\"] = oral_counts[\"Solution\"]\n",
    "route_counts_df[\"Other\"][\"Oral\"] = oral_counts[\"Other\"]\n",
    "\n",
    "# Fa and F\n",
    "bin_edges_fa = np.linspace(min(filtered_df[\"Fa\"]), max(filtered_df[\"Fa\"]), 21)\n",
    "bin_edges_f = np.linspace(min(filtered_df_f[\"F\"]), max(filtered_df_f[\"F\"]), 21)\n",
    "fa_low = filtered_df[filtered_df[\"Fa\"] < 0.8][\"Fa\"]\n",
    "fa_high = filtered_df[filtered_df[\"Fa\"] >= 0.8][\"Fa\"]\n",
    "f_low = filtered_df_f[filtered_df_f[\"F\"] < 0.8][\"F\"]\n",
    "f_high = filtered_df_f[filtered_df_f[\"F\"] >= 0.8][\"F\"]\n",
    "\n",
    "# ATC Codes\n",
    "atc_codes = original_df_[[\"ATC code\", \"Route\"]]\n",
    "atc_codes = atc_codes.dropna()  # Removing agents not assigned an ATC code\n",
    "atc_codes = atc_codes[atc_codes[\"ATC code\"].str.len() < 13] # Removing \"not yet assigned\"\n",
    "atc_codes[\"ATC code\"] = atc_codes[\"ATC code\"].str[0]\n",
    "atc_codes[\"Route\"] = atc_codes[\"Route\"] == \"Oral\"\n",
    "atc_codes[\"Route\"] = atc_codes[\"Route\"].map({True: \"Oral\", False: \"Not Oral\"})\n",
    "atc_codes = atc_codes.groupby([\"ATC code\", \"Route\"]).size().unstack(fill_value=0)\n",
    "atc_codes.columns.name = None  \n",
    "atc_codes.columns = [\"Not oral\", \"Oral\"]\n",
    "atc_codes[\"Total\"] = atc_codes[\"Not oral\"] + atc_codes[\"Oral\"]\n",
    "atc_codes.sort_values(by=\"Total\", inplace=True)\n",
    "atc_codes[\"Category\"]  = atc_codes.apply(lambda row: \"Other\" if row[\"Total\"] < 45 else row.name, axis=1)\n",
    "atc_codes = atc_codes.groupby(\"Category\").sum()\n",
    "atc_codes = atc_codes.sort_values(\"Total\", ascending=False)\n",
    "if \"Other\" in atc_codes.index:\n",
    "    other_row = atc_codes.loc[[\"Other\"]]  # Extract the \"Other\" row\n",
    "    atc_codes = atc_codes.drop(\"Other\")  # Drop the \"Other\" row from the main DataFrame\n",
    "    atc_codes = pd.concat([atc_codes, other_row]) \n",
    "mapping = {\n",
    "    \"L\": \"Antineoplastics and\\nimmunomodulators\",\n",
    "    \"N\": \"Nervous system\",\n",
    "    \"J\": \"Antiinfectives\", \n",
    "    \"A\": \"Alimentary tract\\nand metabolism\",\n",
    "    \"C\": \"Cardiovascular\",\n",
    "    \"B\": \"Blood and blood\\nforming organs\",  \n",
    "    \"R\": \"Respiratory\",\n",
    "    \"G\": \"Genito-urinary system/\\nsex hormones\",\n",
    "    \"Other\": \"Other ATC Codes\"\n",
    "}\n",
    "atc_codes = atc_codes.rename(index=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d21143-48f1-4bb9-8544-d736cb25534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gridspec layout\n",
    "fig = plt.figure(figsize=(17, 10))\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 1], height_ratios=[1, 1], wspace=0.4, hspace=0.4)\n",
    "\n",
    "# Plot 1: Fraction Absorbed\n",
    "ax1 = fig.add_subplot(gs[0, 2])\n",
    "ax1.hist(fa_low, bins=bin_edges_fa, color=\"C0\", edgecolor=\"black\", label=f\"F$_{{a}}$ < 0.8 (n={len(fa_low)})\")\n",
    "ax1.hist(fa_high, bins=bin_edges_fa, color=\"C1\", edgecolor=\"black\", label=f\"F$_{{a}}$ ≥ 0.8 (n={len(fa_high)})\")\n",
    "ax1.set_xlabel(\"Fraction Absorbed\", fontsize=13)\n",
    "legend_patches = [\n",
    "    mpatches.Patch(facecolor=\"C0\", label=f\"F$_{{a}}$ < 0.8 (n={len(fa_low)})\"),\n",
    "    mpatches.Patch(facecolor=\"C1\", label=f\"F$_{{a}}$ ≥ 0.8 (n={len(fa_high)})\")\n",
    "]\n",
    "ax1.legend(handles=legend_patches, fontsize=13)\n",
    "ax1.set_ylabel(\"Frequency\", fontsize=13)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.text(-0.25, 1, \"B\", transform=ax1.transAxes, fontsize=25, fontweight=\"bold\")\n",
    "ax1.tick_params(axis=\"x\", labelsize=11)\n",
    "ax1.tick_params(axis=\"y\", labelsize=11)\n",
    "\n",
    "# Plot 2: Oral Bioavailability\n",
    "ax2 = fig.add_subplot(gs[1, 2])\n",
    "ax2.hist(filtered_df_f[\"F\"], bins=bin_edges_f, color=\"C0\", edgecolor=\"black\")\n",
    "ax2.set_xlabel(\"Oral Bioavailability\", fontsize=13)\n",
    "ax2.set_ylabel(\"Frequency\", fontsize=13)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.text(-0.25, 1, \"D\", transform=ax2.transAxes, fontsize=25, fontweight=\"bold\")\n",
    "ax2.tick_params(axis=\"x\", labelsize=11)\n",
    "ax2.tick_params(axis=\"y\", labelsize=11)\n",
    "\n",
    "# Plot 3: Route of Administration\n",
    "legend_colors = [\"C4\", \"C3\", \"C2\", \"C1\"]\n",
    "legend_labels = [f\"Tablet* (n={route_counts_df[\"Tablet\"].sum()})\", \n",
    "                 f\"Hard Capsule* (n={route_counts_df[\"Hard Capsule\"].sum()})\", \n",
    "                f\"Solution (n={route_counts_df[\"Solution\"].sum()})\", \n",
    "                f\"Other (n={route_counts_df[\"Other\"].sum()})\"]\n",
    "legend_handles = [mpatches.Patch(color=color, label=label) for color, label in zip(legend_colors, legend_labels)]\n",
    "ax3 = fig.add_subplot(gs[0, :2])\n",
    "route_counts_df[::-1].plot(kind=\"barh\", stacked=True, edgecolor=\"black\", ax=ax3, legend=False)\n",
    "ax3.set_ylabel(\"Route\", fontsize=13)\n",
    "ax3.set_xlabel(\"Frequency\", fontsize=13)\n",
    "ax3.grid(axis=\"x\", alpha = 0.5)\n",
    "row_sums = route_counts_df[::-1].sum(axis=1)  \n",
    "bar_positions = range(len(row_sums)) \n",
    "for y, total_count in zip(bar_positions, row_sums):\n",
    "    ax3.text(\n",
    "        total_count + 5,\n",
    "        y,\n",
    "        str(total_count), \n",
    "        va=\"center\", \n",
    "        ha=\"left\", \n",
    "        color=\"black\",\n",
    "        fontsize=13\n",
    "    )\n",
    "ax3.spines[\"right\"].set_visible(False)\n",
    "ax3.spines[\"top\"].set_visible(False)\n",
    "ax3.legend(handles = legend_handles, title=\"Oral Dosage Forms:\", loc=(0.65, 0.2), fontsize=13,  title_fontsize=13)\n",
    "ax3.annotate(\"*Excludes dosage forms specified\\nto have modified release \",  (593, 0.21), fontstyle=\"italic\", fontsize=13)\n",
    "ax3.text(-0.25, 1, \"A\", transform=ax3.transAxes, fontsize=25, fontweight=\"bold\")\n",
    "ax3.tick_params(axis=\"x\", labelsize=11)\n",
    "ax3.tick_params(axis=\"y\", labelsize=11)\n",
    "\n",
    "# Plot 4: Route of Administration 2\n",
    "ax4 = fig.add_subplot(gs[1, :2])\n",
    "atc_codes[[\"Oral\", \"Not oral\"]][::-1].plot(kind=\"barh\", stacked=True,  edgecolor=\"black\", ax=ax4, legend=False)\n",
    "ax4.set_ylabel(\"ATC Level One\\n\", fontsize=13)\n",
    "ax4.set_xlabel(\"Frequency\", fontsize=13)\n",
    "ax4.spines[\"right\"].set_visible(False)\n",
    "ax4.spines[\"top\"].set_visible(False)\n",
    "ax4.grid(axis=\"x\", alpha = 0.5)\n",
    "bars = ax4.patches\n",
    "legend_patches = [\n",
    "    mpatches.Patch(facecolor=\"C0\", label=f\"Oral Route\"),\n",
    "    mpatches.Patch(facecolor=\"C1\", label=f\"Not Oral Route\")\n",
    "]\n",
    "row_sums = atc_codes[[\"Oral\", \"Not oral\"]][::-1].sum(axis=1)  \n",
    "bar_positions = range(len(row_sums)) \n",
    "for y, total_count in zip(bar_positions, row_sums):\n",
    "    ax4.text(\n",
    "        total_count + 2,\n",
    "        y,\n",
    "        str(total_count), \n",
    "        va=\"center\", \n",
    "        ha=\"left\", \n",
    "        color=\"black\",\n",
    "        fontsize=13\n",
    "    )\n",
    "ax4.legend(handles = legend_patches, title=\"Route of Administration:\", loc=(0.65, 0.2), fontsize=13, title_fontsize=13)\n",
    "ax4.tick_params(axis=\"y\", labelsize=8)\n",
    "ax4.text(-0.25, 1, \"C\", transform=ax4.transAxes, fontsize=25, fontweight=\"bold\")\n",
    "ax4.tick_params(axis=\"x\", labelsize=11)\n",
    "ax4.tick_params(axis=\"y\", labelsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/eda.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-dimensional feature set - mordred with RDKit default embedding\n",
    "def get_embedded_mol(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        mol = Chem.AddHs(mol)\n",
    "        AllChem.EmbedMolecule(mol)\n",
    "        AllChem.MMFFOptimizeMolecule(mol)\n",
    "        return mol\n",
    "    except:\n",
    "        return \"Error\"\n",
    "filtered_df[\"Mol\"] = filtered_df[\"p_smiles\"].apply(get_embedded_mol)\n",
    "filtered_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7272197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating all mordered descriptors\n",
    "# Kernel dies if done in the same cell as optimisation\n",
    "calc = Calculator(descriptors, ignore_3D=False)\n",
    "all_desc = calc.pandas(filtered_df[\"Mol\"])\n",
    "all_desc.dropna(axis=1, inplace=True)\n",
    "all_desc = all_desc.select_dtypes(exclude=[\"object\"])\n",
    "X_mordred = all_desc.replace([np.inf, -np.inf], np.nan).dropna(axis=1, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5093279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-dimensional feature set - MACCS Keys\n",
    "maccs_keys_list = []\n",
    "for smiles in filtered_df[\"p_smiles\"]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        maccs_keys = MACCSkeys.GenMACCSKeys(mol)\n",
    "        maccs_keys_list.append(list(maccs_keys.ToBitString()))\n",
    "X_MACCS = pd.DataFrame(maccs_keys_list, columns=[\"MACCS_\" + str(i) for i in range(167)])\n",
    "X_MACCS.replace({\"0\": False, \"1\": True}, inplace=True)\n",
    "\n",
    "\n",
    "# Two-dimensional feature set - RDKit2D\n",
    "generator = MakeGenerator((\"RDKit2D\",))\n",
    "X_RDKit2D = pd.DataFrame([generator.process(smiles) for smiles in filtered_df[\"p_smiles\"]])\n",
    "X_RDKit2D.set_axis([name for name, _ in generator.GetColumns()], axis=1)\n",
    "\n",
    "# ECFP Fingerprints\n",
    "ecfp6_list = []\n",
    "for smiles in filtered_df[\"p_smiles\"]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        ecfp6 = AllChem.GetMorganFingerprintAsBitVect(mol, 3, nBits=2048)  # ECFP6 with radius 3 and 2048 bits\n",
    "        ecfp6_list.append(list(ecfp6.ToBitString()))\n",
    "X_ECFP6 = pd.DataFrame(ecfp6_list, columns=[\"ECFP6_\" + str(i) for i in range(2048)])\n",
    "X_ECFP6.replace({\"0\": False, \"1\": True}, inplace=True)\n",
    "\n",
    "# Lipinski Descriptors\n",
    "def encode_molecule(SMILES_string):\n",
    "    mol = Chem.MolFromSmiles(SMILES_string)\n",
    "    mol_encoding = [Lipinski.FractionCSP3(mol), Lipinski.HeavyAtomCount(mol), Lipinski.NHOHCount(mol),\n",
    "                    Lipinski.NOCount(mol), Lipinski.NumAliphaticCarbocycles(mol), Lipinski.NumAliphaticHeterocycles(mol),\n",
    "                    Lipinski.NumAliphaticRings(mol), Lipinski.NumAromaticCarbocycles(mol), Lipinski.NumAromaticHeterocycles(mol),\n",
    "                    Lipinski.NumAromaticRings(mol), Lipinski.NumHAcceptors(mol), Lipinski.NumHDonors(mol),\n",
    "                    Lipinski.NumHeteroatoms(mol), Lipinski.NumRotatableBonds(mol), Lipinski.NumSaturatedCarbocycles(mol),\n",
    "                    Lipinski.NumSaturatedHeterocycles(mol), Lipinski.NumSaturatedRings(mol), Lipinski.RingCount(mol)]\n",
    "    return mol_encoding\n",
    "data1 = filtered_df[\"p_smiles\"]\n",
    "feature_list = []\n",
    "for smile in data1:\n",
    "    feature_list.append(encode_molecule(smile))\n",
    "data2 = np.array(feature_list)\n",
    "X_lipinski = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004355df-ec9b-432b-9e3e-0c51b88aca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "classifiers = {\n",
    "    \"Random Forest Classifier\": {\n",
    "        \"model\": RandomForestClassifier(random_state=0),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [25, 50, 100],\n",
    "            \"max_depth\": [4, 8],\n",
    "            \"min_samples_split\": [2, 3, 5],\n",
    "            \"min_samples_leaf\": [1, 2, 4],\n",
    "            \"max_features\": [None, \"sqrt\"] # 108 combinations\n",
    "        }\n",
    "      },    \n",
    "    \"AdaBoost Classifier\": {\n",
    "        \"model\": AdaBoostClassifier(random_state=0, algorithm=\"SAMME\"),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [25, 50, 100],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.5], # 9 combiantions\n",
    "        }\n",
    "   },\n",
    "    \"Gradient Boosting Machine\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=0),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [25, 50, 100],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.5],\n",
    "            \"max_depth\": [2, 4, 8] # 27 combinations\n",
    "       }\n",
    "    },\n",
    "    \"Decision Tree Classifier\": {\n",
    "    \"model\": DecisionTreeClassifier(random_state=0),\n",
    "    \"param_grid\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [2, 4, 8],\n",
    "        \"min_samples_split\": [2, 4, 8],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [None, \"sqrt\"], # 108 combinations\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e56fc-d1f4-41c7-bccd-348dc717e6ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining X and Y\n",
    "filtered_df[\"0.8_cutoff\"] = filtered_df[\"Fa\"].apply(lambda x: True if x >= 0.8 else False)\n",
    "y = filtered_df[\"0.8_cutoff\"]\n",
    "X_list = [X_MACCS, X_ECFP6, X_lipinski, X_RDKit2D, X_mordred]\n",
    "dataset_names = [\"MACCS\", \"ECFP6\", \"Lipinski\", \"RDKit2D\", \"Mordred\"]\n",
    "results_list = []\n",
    "\n",
    "# Grid search of hyperparameters, followed by refitting on all the data for the model which should the highest cross-validation balanced accuracy\n",
    "# and obtaining final train and test perfomance metrics. Note that the best model was not selected on the basis of test set performance, but\n",
    "# cross-validation performance. \n",
    "\n",
    "for X, dataset in zip(X_list, dataset_names):\n",
    "    print(f\"Descriptor set: {dataset}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0) \n",
    "    for name, classifier in classifiers.items():\n",
    "        print(f\"Algorithm name: {name}\")\n",
    "        model = classifier[\"model\"]\n",
    "        param_grid = classifier[\"param_grid\"]\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring=\"balanced_accuracy\", cv=skf, n_jobs=-1) # Parallel to reduce computation time\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        top_results = cv_results.sort_values(\"mean_test_score\", ascending=False).head(1)\n",
    "        top_results[\"descriptor_set\"] = dataset\n",
    "        top_results[\"classifier\"] = name\n",
    "        top_results[\"train_ba\"] = balanced_accuracy_score(y_train, y_pred_train)\n",
    "        top_results[\"test_ba\"] = balanced_accuracy_score(y_test, y_pred_test)\n",
    "        results_list.append(top_results)\n",
    "\n",
    "final_cv_df = pd.concat(results_list, ignore_index=True)\n",
    "final_cv_df.sort_values(by=[\"mean_test_score\"], ascending=[False], inplace=True) # Best model should be selected on basis of CV performance\n",
    "final_cv_df.drop_duplicates(subset=\"descriptor_set\", keep=\"first\", inplace=True) # Only considering best model for each descriptor set\n",
    "final_cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bd925-f683-49e4-9ebb-e8566e040f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cv_df.to_csv(\"../results/cross_validation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485e438-3edd-407d-802b-209ab6d5f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Table 2. Mordred may be variable between runs due to embedding.\n",
    "final_cv_df[[\"descriptor_set\", \"classifier\", \"mean_test_score\", \"std_test_score\", \"train_ba\", \"test_ba\"]].round(3)\n",
    "# Generating LaTeX for manuscript\n",
    "# print(final_cv_df[[\"descriptor_set\", \"classifier\", \"mean_test_score\", \"std_test_score\", \"train_ba\", \"test_ba\"]].round(3).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ab08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional info for tree classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_RDKit2D, y, test_size=0.2, stratify=y, random_state=0) \n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "param_grid = {\n",
    "        \"criterion\": [\"entropy\"],\n",
    "        \"max_depth\": [8],\n",
    "        \"min_samples_split\": [8],\n",
    "        \"min_samples_leaf\": [1],\n",
    "        \"max_features\": [\"sqrt\"],\n",
    "        }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"balanced_accuracy\", cv=skf, n_jobs=-1) # Parallel to reduce computation time\n",
    "grid_search.fit(X_train, y_train)\n",
    "tree_cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "\n",
    "test_accuracy = balanced_accuracy_score(y_test, y_pred_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "train_accuracy = balanced_accuracy_score(y_train, y_pred_train)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(conf_matrix)\n",
    "\n",
    "TN, FP = conf_matrix[0]\n",
    "FN, TP = conf_matrix[1]\n",
    "\n",
    "SE = TP / (TP + FN)\n",
    "SP = TN / (TN + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "\n",
    "print(f\"Sensitivity (SE): {SE:.2f}\")\n",
    "print(f\"Specificity (SP): {SP:.2f}\")\n",
    "print(f\"Positive Predictive Value (PPV): {PPV:.2f}\")\n",
    "print(f\"Negative Predictive Value (NPV): {NPV:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e649614-5d23-412e-bbe5-2557b26fedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best model\n",
    "tree_text = export_text(best_model, feature_names=generator.GetColumns())\n",
    "with open(\"../results/decision_tree_description.txt\", \"w\") as f:\n",
    "    f.write(tree_text)\n",
    "with open(\"../results/best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_model, filled=True, feature_names=generator.GetColumns())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed8943-126b-4b1d-978b-8a94e3b09566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_assignments = pd.concat([X_RDKit2D, y], axis=1)\n",
    "class_assignments[[\"drug_substance\", \"p_smiles\"]] = filtered_df[[\"actives_in_dosage_form\", \"p_smiles\"]]\n",
    "class_assignments[\"Set\"] = \"Train\"\n",
    "class_assignments.loc[y_test.index, \"Set\"] = \"Test\"\n",
    "class_assignments[\"Pred\"] = None\n",
    "class_assignments.loc[y_train.index, \"Pred\"] = y_pred_train\n",
    "class_assignments.loc[y_test.index, \"Pred\"] = y_pred_test\n",
    "class_assignments.to_csv(\"../results/train_test_split_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01527f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified slightly from code made available by https://github.com/molecularmodelinglab/MODI/tree/main\n",
    "# Used in Rath et al. JMedChem doi/10.1021/acs.jmedchem.3c02446\n",
    "\n",
    "def nearest_neighbors(reference, query, k=1, self_query=False):\n",
    "    tree = sp.KDTree(reference)\n",
    "    if self_query:\n",
    "        k = [x+2 for x in range(k)]\n",
    "    else:\n",
    "        k = [x+1 for x in range(k)]\n",
    "    _, i = tree.query(query, k=k, workers=-1)\n",
    "    return i\n",
    "\n",
    "def modi(data, labels):\n",
    "    classes = np.unique(labels)\n",
    "    k = classes.shape[0]\n",
    "    nn_idx = nearest_neighbors(data, data, k=1, self_query=True)\n",
    "    nn_labels = labels[nn_idx]\n",
    "    modi_value = 0\n",
    "    class_contrib = []\n",
    "    for c in classes:\n",
    "        c_arr = np.where(labels == c)[0]\n",
    "        c_labels = labels[c_arr]\n",
    "        c_nn_labels = nn_labels[c_arr].flatten()\n",
    "        modi_value += np.sum(c_labels == c_nn_labels) / c_arr.shape[0]\n",
    "        class_contrib.append((c, np.sum(c_labels == c_nn_labels) / c_arr.shape[0]))\n",
    "    return (k ** -1) * modi_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling is important as modelability index is based on nearest neighbour in Euclidean distance. \n",
    "# Scaling should in theory not affect tree performance as it is based on entropy/mutual info/\n",
    "# some other metric of performance.\n",
    "\n",
    "scaled_rdkit = StandardScaler().fit_transform(X_RDKit2D)\n",
    "scaled_mordred = StandardScaler().fit_transform(X_mordred)\n",
    "scaled_lipinski = StandardScaler().fit_transform(X_lipinski)\n",
    "y_numpy = y.to_numpy()\n",
    "\n",
    "print(f\"MACCS: {modi(X_MACCS.to_numpy(), y_numpy)}\")\n",
    "print(f\"ECFP6: {modi(X_ECFP6.to_numpy(), y_numpy)}\")\n",
    "print(f\"RDKit2D: {modi(scaled_rdkit, y_numpy)}\")\n",
    "print(f\"Lipinski: {modi(scaled_lipinski, y_numpy)}\")\n",
    "print(f\"Mordred: {modi(scaled_mordred, y_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b0916-678f-46c1-8560-ca20cefca6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=generator.GetColumns()\n",
    "X_RDKit2D.columns = [ x for x, _ in feature_names]\n",
    "spearman_corr = abs(X_RDKit2D.corr(method=\"spearman\")[\"Chi0n\"])\n",
    "spearman_corr[spearman_corr > 0.9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
